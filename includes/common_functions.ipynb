{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0678810c-d027-4aec-af67-4efd93a0956f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add ingestion date column to each data frame using current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b404a03-9fba-47bb-b7ce-3a676b1729e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def add_ingestion_date(input_df):\n",
    "    output_df = input_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12489e40-ba26-4d76-b1f5-ebb46a230eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Make a list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4397823d-6dbf-4632-aade-3ed47cb614b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_column_to_list(input_df, column_name):\n",
    "    df_row_list = input_df.select(column_name).distinct().collect()\n",
    "\n",
    "    column_value_list = [row[column_name] for row in df_row_list]\n",
    "    return column_value_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de19fe2-6d98-43cd-9907-2615df001fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Delta Lake table or insert/update records to an existing Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7af967b-1310-4437-bec2-c440b4221def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n",
    "    # Set dynamic partition pruning to true to avoid full table scan by using race_id to find the partition\n",
    "    spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\")\n",
    "\n",
    "    # If the table exists, update the records from the source df that already exist in the target table, or insert the new records that are not matched. If the table does not exist, create the table and populate the data.\n",
    "    from delta.tables import DeltaTable\n",
    "    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "        deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "        deltaTable.alias(\"tgt\").merge(\n",
    "            input_df.alias(\"src\"),\n",
    "            merge_condition) \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "    else:\n",
    "        input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}